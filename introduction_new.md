\section{Introduction}
\IEEEPARstart{I}{n} the global background of Industry 4.0 and intelligent manufacturing, process industries—encompassing core sectors such as chemical engineering, petroleum refining, non-ferrous metallurgy, and wastewater treatment—are undergoing a profound paradigm shift from automation to intelligence. The driving force behind this transformation lies in the effective utilization of massive industrial process data, integrating domain knowledge, data science, and artificial intelligence to achieve precise perception, real-time optimization, and intelligent decision-making. In this context, soft sensing technology has emerged as a cornerstone of process monitoring. By utilizing easy-to-measure auxiliary variables to infer Key Quality Indicators in real-time, soft sensing effectively resolves the challenge of significant time delays associated with traditional offline analysis, thereby greatly enhancing the responsiveness and stability of control systems.

However, despite significant achievements in theoretical research and preliminary applications, the large-scale, long-term deployment of soft sensors in industrial scenarios remains hindered by a severe and pervasive challenge: non-stationarity. Industrial environments are not idealized static systems but complex, time-varying environments characterized by dynamic evolution. Over time, the interplay of various physical and chemical factors—such as catalyst deactivation, equipment fouling, fluctuations in raw material properties, and external environmental changes—induces concept drift in the statistical characteristics of process data. This drift causes static global models trained on historical data to gradually deviate from actual operating conditions, leading to the phenomenon known as "soft sensor aging," which severely impedes the sustainable development of industrial intelligence. Crucially, the non-stationarity induced by concept drift manifests in complex, multi-dimensional forms rather than as a singular statistical phenomenon. In real-world industrial sites, these drifts exhibit diverse time scales and statistical features, ranging from transient abrupt shifts to long-term gradual evolutions, imposing distinct and stringent adaptability requirements on monitoring models. Table \ref{table1} systematically summarizes five typical types of concept drift in industrial processes along with their corresponding physical mechanisms.

To address the diverse drifts illustrated in Table \ref{table1}, research focus in both academia and industry has shifted from static modeling to dynamic techniques capable of online adaptation. Ideally, a soft sensor must navigate the classic "Stability-Plasticity Dilemma": it requires sufficient plasticity to rapidly adapt to new regimes while maintaining high stability to prevent catastrophic forgetting of historical knowledge. Early adaptive approaches were primarily based on recursive identification theory, typified by Recursive Least Squares (RLS) and its kernel variants (KRLS). These methods employ forgetting factors to dynamically adjust memory length, attempting to balance tracking agility with noise suppression. However, simple recursive updates often struggle to capture the strong nonlinear dynamics of complex industrial processes and are susceptible to interference from high-dimensional collinear data.

To tackle both nonlinearity and non-stationarity, the Just-In-Time Learning (JITL) paradigm emerged. JITL operates on the "local linearity" assumption: while the global process is nonlinear and time-varying, it can be approximated as linear within a small local neighborhood. Consequently, JITL abandons the maintenance of a single global model in favor of constructing local models on-the-fly for each query sample. The efficacy of JITL hinges on the definition of similarity. The traditional Euclidean distance often fails to account for the multivariate coupling of industrial data. To address this, recent works have introduced spatio-temporal graph decoupling [12] to incorporate causal relationships and topological consistency. Similarly, Dynamic Time Warping (DTW) [13] has been adopted to handle nonlinear temporal distortions, enabling the capture of transitional dynamics. Despite its success in handling nonlinearity, JITL's reliance on simple input similarity often overlooks the evolving input-output mapping itself, leaving it vulnerable in highly non-stationary regimes.

With the rapid advancement of the Industrial Internet of Things (IIoT), the emergence of massive high-dimensional data has propelled the application of deep learning in soft sensing. To adapt to dynamic environments, incremental learning has been introduced into deep neural networks, aiming to accommodate new data streams by fine-tuning network parameters. However, this naive adaptation faces a severe "Stability-Plasticity Dilemma": the rapid acquisition of new knowledge often leads to the catastrophic forgetting of previously learned experiences.
To mitigate this issue, a growing body of research on Continual Learning (CL) has emerged, predominantly in the field of process monitoring. One representative category of approaches employs parameter isolation or structural expansion strategies. For instance, Huang et al. proposed an Incremental Rank Continual Dictionary Learning (IRCDL) framework. By abandoning a single shared model update strategy, they introduced a low-rank matrix augmentation mechanism that trains and freezes independent parameters for each new mode, thereby achieving "zero-forgetting" monitoring of multimode processes. Another mainstream category relies on regularization-based strategies. Zhang et al. integrated Elastic Weight Consolidation (EWC) into Probabilistic Slow Feature Analysis (PSFA) and Recursive Principal Component Analysis (RPCA). By evaluating parameter importance via the Fisher information matrix and applying quadratic penalties, these methods effectively constrain the drift of critical parameters while updating the model to capture time-varying dynamics—such as slow features or long-term cointegration relationships. Consequently, they achieve a dynamic balance between preserving historical memory and adapting to new operating conditions. 

While the aforementioned continual learning-based studies on process monitoring and fault diagnosis have made significant strides in handling the drifts listed in Table 1, it must be noted that these works primarily address Classification or Anomaly Detection tasks. A significant gap remains in online learning research for Soft Sensing—specifically, Regression tasks for non-stationary time series. Compared to classification, regression tasks face more severe and fundamentally different challenges when confronting the drifts described in Table 1, as detailed below:
\begin{itemize}
    \item \textbf{Sensitivity to Gradual and Incremental Drift:} In process monitoring (classification) tasks, models benefit from a large margin of error. For gradual or incremental drifts (Table 1), such as heat exchanger fouling, as long as the distribution shift does not cross the inter-class Decision Boundary, the prediction typically remains robust. However, in soft sensing (regression), the continuity of the output space eliminates this tolerance buffer. Any minute gradual shift in input distribution (Covariate Shift) or mapping relationship (Concept Shift) translates directly and linearly into prediction Bias. Existing regularization strategies, which focus on maintaining class separability, lack mechanisms for the high-precision real-time correction of continuous numerical deviations.
    \item \textbf{Continuous Evolution vs. Discrete Modes in Reoccurring Drift:} For periodic or reoccurring drifts, existing continual learning methods typically employ "model pool" or "parameter subspace" strategies, discretizing the process into fixed modes. While effective for fault classification, industrial soft sensing targets often reside on a continuously changing Dynamic Manifold. For instance, during multi-grade product transitions, the process undergoes continuous Transition States rather than hopping between discrete modes. Directly applying "discrete mode memory" mechanisms leads to model failure during these transition intervals. Therefore, regression tasks demand an architecture capable of adapting to Continuous Dynamics rather than mere discrete mode switching.
    \item \textbf{Robustness to Open-world Drift:} Facing open-world drifts (e.g., new raw materials), existing monitoring methods need only identify the state as "unknown" (open-set recognition). However, soft sensing systems cannot stop at rejection; they must provide minimally risky predictions. Naive adaptation methods (like Test-Time Training) often fail catastrophically here by over-fitting to the novel distribution with insufficient samples. A robust system requires a "fail-safe" mechanism—relying on the physics-informed backbone when retrieval confidence is low—rather than blindly adapting to every anomaly.
    \item \textbf{Contextual Ambiguity (State Aliasing) in Retrieval:} Most retrieval-based adaptive methods (e.g., JITL) rely solely on input similarity to locate historical reference samples. However, in non-stationary industrial systems, the mapping from inputs to outputs is one-to-many depending on the latent regime (e.g., catalyst activity level). Two time steps may exhibit identical sensor readings but yield divergent quality outcomes due to underlying parameter shifts. Relying exclusively on input similarity causes \textbf{State Aliasing}, leading to the retrieval of contextually mismatched error patterns (Negative Transfer). Effective adaptation requires resolving this ambiguity by incorporating the model's predictive intent as a latent context descriptor.
\end{itemize}

In summary, directly applying continual learning strategies designed for classification fails to meet the stringent requirements of soft sensing for numerical precision, continuous tracking, and unknown robustness. Consequently, there is an urgent need to develop an online learning mechanism tailored specifically for the characteristics of time-series regression.

To address these challenges, we propose \textbf{H-Mem}, a novel \textbf{Horizon-Bridging Contextual Memory} framework. Unlike gradient-based methods that struggle with the stability-plasticity trade-off, H-Mem adopts a non-parametric "Retrieve-and-Correct" paradigm. Our core innovation is the \textbf{Dual-Key Contextual Retrieval} mechanism. We postulate that the model's own \textit{predictive intent} explicitly encodes the current regime. By fusing the environmental state (Input) with the model's intent (Base Prediction) into a composite query, H-Mem disambiguates aliased states, ensuring that historical experiences are retrieved only when both the \textit{situation} and the \textit{response} align. Furthermore, to respect strict causality, we implement a \textbf{Delayed Feedback Evolution} loop that updates the memory only when ground truth becomes available ($t+H$).

The main contributions of this paper are summarized as follows:
\begin{itemize}
    \item \textbf{H-Mem Framework:} We propose a decoupled architecture combining a Frozen Backbone for stability and a \textbf{Dual-Key Retrieval Corrector (DK-RC)} for plasticity. This structure resolves the conflict between retaining universal physical laws and tracking local drifts.
    \item \textbf{Resolving State Aliasing:} We identify the "State Aliasing" problem in industrial retrieval adaptation and propose a \textbf{Dual-Key} mechanism (Input + Prediction) to eliminate contextual ambiguity, significantly reducing negative transfer in multi-regime processes.
    \item \textbf{Regime-Aware Robustness:} We introduce \textbf{Regime-Aware Bucketing} and \textbf{Adaptive Confidence Gating} to handle continuous dynamics and filter noise. This ensures the model adapts aggressively in familiar regimes while robustly falling back to the backbone in open-world scenarios.
    \item \textbf{Industrial Validation:} Extensive experiments on standard benchmarks and real-world \textbf{Grinding} and \textbf{Flotation} processes demonstrate that H-Mem achieves superior accuracy and efficiency compared to SOTA continual learning methods, offering a strictly causal, gradient-free solution for online soft sensing.
\end{itemize}
