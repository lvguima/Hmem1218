"""
LoRA (Low-Rank Adaptation) layers for H-Mem.

Key difference from standard LoRA:
- Parameters A, B are GENERATED by HyperNetwork, not directly learned
- Supports dynamic injection at each forward pass
- Compatible with existing OnlineTSF adapter architecture

Author: H-Mem Implementation
Date: 2025-12-13
"""

import math
from typing import Optional, Tuple, List, Dict, Union

import torch
import torch.nn as nn
import torch.nn.functional as F


class LoRALayer:
    """
    Mixin class providing LoRA functionality.

    LoRA decomposes weight updates as: ΔW = A @ B
    where A ∈ R^{r×d_in}, B ∈ R^{d_out×r}, and r << min(d_in, d_out)
    """

    def __init__(
        self,
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0,
        merge_weights: bool = False
    ):
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank
        self.dropout_rate = dropout
        self.merge_weights = merge_weights

        # LoRA parameters (dynamically set)
        self.lora_A: Optional[torch.Tensor] = None  # [rank, in_features]
        self.lora_B: Optional[torch.Tensor] = None  # [out_features, rank]

        # Merged state tracking
        self.merged = False

    def set_lora_params(
        self,
        lora_A: torch.Tensor,
        lora_B: torch.Tensor
    ):
        """
        Inject LoRA parameters from HyperNetwork.

        Args:
            lora_A: [batch, rank, in_features] or [rank, in_features]
            lora_B: [batch, out_features, rank] or [out_features, rank]
        """
        self.lora_A = lora_A
        self.lora_B = lora_B
        self.merged = False

    def clear_lora_params(self):
        """Reset LoRA parameters (for inference without adaptation)."""
        self.lora_A = None
        self.lora_B = None
        self.merged = False

    def has_lora_params(self) -> bool:
        """Check if LoRA parameters are currently set."""
        return self.lora_A is not None and self.lora_B is not None

    @property
    def lora_param_count(self) -> int:
        """Total number of LoRA parameters for this layer."""
        raise NotImplementedError


class LoRALinear(nn.Linear, LoRALayer):
    """
    Linear layer with LoRA adaptation capability.

    Forward: y = x @ (W + scaling * A @ B)^T + bias
           = x @ W^T + scaling * x @ B^T @ A^T + bias
           = original_output + lora_delta

    Attributes:
        in_features: Input dimension
        out_features: Output dimension
        rank: LoRA rank (r)
        weight: Frozen original weight [out_features, in_features]
        bias: Optional frozen bias [out_features]
        lora_A: Dynamic LoRA A matrix [rank, in_features]
        lora_B: Dynamic LoRA B matrix [out_features, rank]
        scaling: Scaling factor (alpha / rank)
    """

    def __init__(
        self,
        in_features: int,
        out_features: int,
        bias: bool = True,
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0,
        merge_weights: bool = False,
        freeze_weight: bool = True,
        device=None,
        dtype=None
    ):
        nn.Linear.__init__(self, in_features, out_features, bias=bias, device=device, dtype=dtype)
        LoRALayer.__init__(self, rank=rank, alpha=alpha, dropout=dropout, merge_weights=merge_weights)

        # Freeze original weights
        self.weight.requires_grad = not freeze_weight
        if self.bias is not None:
            self.bias.requires_grad = not freeze_weight

        # Dropout for regularization
        self.lora_dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    @property
    def lora_param_count(self) -> int:
        """Total number of LoRA parameters: rank * (in + out)"""
        return self.rank * (self.in_features + self.out_features)

    def get_lora_param_shapes(self) -> Dict[str, Tuple[int, ...]]:
        """Get shapes for LoRA parameters."""
        return {
            'A': (self.rank, self.in_features),
            'B': (self.out_features, self.rank)
        }

    def _compute_lora_output(self, x: torch.Tensor) -> torch.Tensor:
        """
        Compute the LoRA delta output.

        Args:
            x: Input tensor [batch, ..., in_features]

        Returns:
            LoRA output [batch, ..., out_features]
        """
        x_dropped = self.lora_dropout(x)

        # Check if batched LoRA parameters
        if self.lora_A.dim() == 3:  # [batch, rank, in_features]
            # Per-sample adaptation
            batch_size = x.size(0)
            original_shape = x.shape

            # Flatten to [batch, seq_len, in_features]
            x_flat = x_dropped.view(batch_size, -1, self.in_features)

            # x @ A^T: [batch, seq, in] @ [batch, in, rank] -> [batch, seq, rank]
            lora_mid = torch.bmm(x_flat, self.lora_A.transpose(-2, -1))

            # lora_mid @ B^T: [batch, seq, rank] @ [batch, rank, out] -> [batch, seq, out]
            lora_output = torch.bmm(lora_mid, self.lora_B.transpose(-2, -1))

            # Reshape back to original shape
            output_shape = original_shape[:-1] + (self.out_features,)
            lora_output = lora_output.view(output_shape)

        else:  # [rank, in_features] - shared across batch
            # x @ A^T @ B^T = x @ (B @ A)^T
            # More efficient: compute in two steps
            lora_output = F.linear(F.linear(x_dropped, self.lora_A), self.lora_B)

        return self.scaling * lora_output

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with optional LoRA adaptation.

        Args:
            x: Input tensor [batch, ..., in_features]

        Returns:
            Output tensor [batch, ..., out_features]
        """
        # Original linear transformation
        result = F.linear(x, self.weight, self.bias)

        # Apply LoRA delta if parameters are set
        if self.has_lora_params():
            result = result + self._compute_lora_output(x)

        return result

    def extra_repr(self) -> str:
        return (f'in_features={self.in_features}, out_features={self.out_features}, '
                f'bias={self.bias is not None}, rank={self.rank}, scaling={self.scaling:.2f}')


class LoRAConv1d(nn.Conv1d, LoRALayer):
    """
    Conv1d layer with LoRA adaptation (for Transformer's Conv projections).

    Only supports kernel_size=1 (common in Transformers) for simplicity,
    treating the convolution as a linear transformation.
    """

    def __init__(
        self,
        in_channels: int,
        out_channels: int,
        kernel_size: int = 1,
        stride: int = 1,
        padding: int = 0,
        dilation: int = 1,
        groups: int = 1,
        bias: bool = True,
        padding_mode: str = 'zeros',
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0,
        merge_weights: bool = False,
        freeze_weight: bool = True,
        device=None,
        dtype=None
    ):
        nn.Conv1d.__init__(
            self, in_channels, out_channels, kernel_size,
            stride=stride, padding=padding, dilation=dilation,
            groups=groups, bias=bias, padding_mode=padding_mode,
            device=device, dtype=dtype
        )
        LoRALayer.__init__(self, rank=rank, alpha=alpha, dropout=dropout, merge_weights=merge_weights)

        # Only support kernel_size=1 for simplicity
        if kernel_size != 1:
            raise ValueError(f"LoRAConv1d only supports kernel_size=1, got {kernel_size}")

        # Freeze original weights
        self.weight.requires_grad = not freeze_weight
        if self.bias is not None:
            self.bias.requires_grad = not freeze_weight

        self.lora_dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

    @property
    def lora_param_count(self) -> int:
        return self.rank * (self.in_channels + self.out_channels)

    def get_lora_param_shapes(self) -> Dict[str, Tuple[int, ...]]:
        return {
            'A': (self.rank, self.in_channels),
            'B': (self.out_channels, self.rank)
        }

    def _compute_lora_output(self, x: torch.Tensor) -> torch.Tensor:
        """
        Compute LoRA delta for Conv1d.

        Args:
            x: [batch, in_channels, length]

        Returns:
            [batch, out_channels, length]
        """
        x_dropped = self.lora_dropout(x)

        # Transpose to [batch, length, in_channels] for linear ops
        x_t = x_dropped.transpose(1, 2)

        if self.lora_A.dim() == 3:  # [batch, rank, in_channels]
            batch_size = x.size(0)

            # x_t @ A^T: [batch, len, in] @ [batch, in, rank] -> [batch, len, rank]
            lora_mid = torch.bmm(x_t, self.lora_A.transpose(-2, -1))

            # lora_mid @ B^T: [batch, len, rank] @ [batch, rank, out] -> [batch, len, out]
            lora_out = torch.bmm(lora_mid, self.lora_B.transpose(-2, -1))

        else:  # [rank, in_channels]
            lora_out = F.linear(F.linear(x_t, self.lora_A), self.lora_B)

        # Transpose back to [batch, out_channels, length]
        return self.scaling * lora_out.transpose(1, 2)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, in_channels, length]
        Returns:
            [batch, out_channels, length]
        """
        result = self._conv_forward(x, self.weight, self.bias)

        if self.has_lora_params():
            result = result + self._compute_lora_output(x)

        return result

    def extra_repr(self) -> str:
        s = super().extra_repr()
        return s + f', rank={self.rank}, scaling={self.scaling:.2f}'


class LoRAEmbedding(nn.Embedding, LoRALayer):
    """
    Embedding layer with LoRA adaptation.

    Useful for adapting token embeddings in Transformer models.
    """

    def __init__(
        self,
        num_embeddings: int,
        embedding_dim: int,
        padding_idx: Optional[int] = None,
        rank: int = 8,
        alpha: float = 16.0,
        merge_weights: bool = False,
        freeze_weight: bool = True,
        device=None,
        dtype=None
    ):
        nn.Embedding.__init__(
            self, num_embeddings, embedding_dim,
            padding_idx=padding_idx, device=device, dtype=dtype
        )
        LoRALayer.__init__(self, rank=rank, alpha=alpha, merge_weights=merge_weights)

        self.weight.requires_grad = not freeze_weight

    @property
    def lora_param_count(self) -> int:
        return self.rank * (self.num_embeddings + self.embedding_dim)

    def get_lora_param_shapes(self) -> Dict[str, Tuple[int, ...]]:
        return {
            'A': (self.rank, self.num_embeddings),
            'B': (self.embedding_dim, self.rank)
        }

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        result = F.embedding(
            x, self.weight, self.padding_idx, self.max_norm,
            self.norm_type, self.scale_grad_by_freq, self.sparse
        )

        if self.has_lora_params():
            # x is indices [batch, seq], we need one-hot then matmul
            # Actually embedding lookup is equivalent to selecting rows
            if self.lora_A.dim() == 3:
                # Batched case - more complex, skip for now
                pass
            else:
                # A: [rank, num_embeddings], B: [embedding_dim, rank]
                # For each index, we select A[:, idx] and multiply by B
                lora_embed = F.embedding(x, self.lora_A.T)  # [batch, seq, rank]
                lora_output = F.linear(lora_embed, self.lora_B)  # [batch, seq, embedding_dim]
                result = result + self.scaling * lora_output

        return result


# =============================================================================
# Factory Functions
# =============================================================================

def create_lora_layer_from_linear(
    original_layer: nn.Linear,
    rank: int = 8,
    alpha: float = 16.0,
    dropout: float = 0.0,
    freeze_weight: bool = True
) -> LoRALinear:
    """
    Create a LoRALinear layer from an existing nn.Linear layer.

    Args:
        original_layer: Source linear layer
        rank: LoRA rank
        alpha: LoRA scaling factor
        dropout: Dropout rate
        freeze_weight: Whether to freeze original weights

    Returns:
        LoRALinear with copied weights
    """
    lora_layer = LoRALinear(
        in_features=original_layer.in_features,
        out_features=original_layer.out_features,
        bias=original_layer.bias is not None,
        rank=rank,
        alpha=alpha,
        dropout=dropout,
        freeze_weight=freeze_weight,
        device=original_layer.weight.device,
        dtype=original_layer.weight.dtype
    )

    # Copy weights
    lora_layer.weight.data.copy_(original_layer.weight.data)
    if original_layer.bias is not None:
        lora_layer.bias.data.copy_(original_layer.bias.data)

    return lora_layer


def create_lora_layer_from_conv1d(
    original_layer: nn.Conv1d,
    rank: int = 8,
    alpha: float = 16.0,
    dropout: float = 0.0,
    freeze_weight: bool = True
) -> LoRAConv1d:
    """
    Create a LoRAConv1d layer from an existing nn.Conv1d layer.

    Args:
        original_layer: Source conv1d layer (must have kernel_size=1)
        rank: LoRA rank
        alpha: LoRA scaling factor
        dropout: Dropout rate
        freeze_weight: Whether to freeze original weights

    Returns:
        LoRAConv1d with copied weights
    """
    if original_layer.kernel_size[0] != 1:
        raise ValueError(f"Only kernel_size=1 is supported, got {original_layer.kernel_size}")

    lora_layer = LoRAConv1d(
        in_channels=original_layer.in_channels,
        out_channels=original_layer.out_channels,
        kernel_size=1,
        stride=original_layer.stride[0],
        padding=original_layer.padding[0],
        dilation=original_layer.dilation[0],
        groups=original_layer.groups,
        bias=original_layer.bias is not None,
        padding_mode=original_layer.padding_mode,
        rank=rank,
        alpha=alpha,
        dropout=dropout,
        freeze_weight=freeze_weight,
        device=original_layer.weight.device,
        dtype=original_layer.weight.dtype
    )

    # Copy weights
    lora_layer.weight.data.copy_(original_layer.weight.data)
    if original_layer.bias is not None:
        lora_layer.bias.data.copy_(original_layer.bias.data)

    return lora_layer


# =============================================================================
# LoRA Injection Utilities
# =============================================================================

def get_lora_param_dims(
    model: nn.Module,
    rank: int = 8,
    target_modules: Optional[List[str]] = None
) -> Dict[str, Dict[str, Union[Tuple[int, ...], int]]]:
    """
    Calculate LoRA parameter dimensions for each layer in the model.

    Args:
        model: The model to analyze
        rank: LoRA rank
        target_modules: List of module name patterns to include (None = all)

    Returns:
        Dict mapping layer names to {'A': shape, 'B': shape, 'total': count}
    """
    def should_include(name: str) -> bool:
        if target_modules is None:
            return True
        return any(pattern in name for pattern in target_modules)

    dims = {}
    for name, module in model.named_modules():
        if not should_include(name):
            continue

        if isinstance(module, nn.Linear):
            dims[name] = {
                'A': (rank, module.in_features),
                'B': (module.out_features, rank),
                'total': rank * (module.in_features + module.out_features),
                'type': 'linear'
            }
        elif isinstance(module, nn.Conv1d) and module.kernel_size[0] == 1:
            dims[name] = {
                'A': (rank, module.in_channels),
                'B': (module.out_channels, rank),
                'total': rank * (module.in_channels + module.out_channels),
                'type': 'conv1d'
            }

    return dims


def inject_lora_layers(
    model: nn.Module,
    rank: int = 8,
    alpha: float = 16.0,
    dropout: float = 0.0,
    target_modules: Optional[List[str]] = None,
    freeze_weight: bool = True
) -> Tuple[nn.Module, Dict[str, Dict]]:
    """
    Replace Linear/Conv1d layers with LoRA-enabled versions.

    Args:
        model: Original model
        rank: LoRA rank
        alpha: LoRA scaling factor
        dropout: Dropout rate for LoRA
        target_modules: List of module name patterns to inject (None = all)
        freeze_weight: Whether to freeze original weights

    Returns:
        Tuple of (modified model, dict of lora layer info)
    """
    def should_replace(name: str) -> bool:
        if target_modules is None:
            return True
        return any(pattern in name for pattern in target_modules)

    lora_layers_info = {}

    # Collect parent-child relationships
    replacements = []
    for name, module in model.named_modules():
        if not should_replace(name):
            continue

        if isinstance(module, nn.Linear):
            replacements.append((name, 'linear', module))
        elif isinstance(module, nn.Conv1d) and module.kernel_size[0] == 1:
            replacements.append((name, 'conv1d', module))

    # Perform replacements
    for name, layer_type, old_module in replacements:
        # Navigate to parent
        parts = name.split('.')
        parent = model
        for part in parts[:-1]:
            parent = getattr(parent, part)
        child_name = parts[-1]

        # Create LoRA layer
        if layer_type == 'linear':
            new_module = create_lora_layer_from_linear(
                old_module, rank=rank, alpha=alpha,
                dropout=dropout, freeze_weight=freeze_weight
            )
        else:  # conv1d
            new_module = create_lora_layer_from_conv1d(
                old_module, rank=rank, alpha=alpha,
                dropout=dropout, freeze_weight=freeze_weight
            )

        # Replace module
        setattr(parent, child_name, new_module)

        # Record info
        lora_layers_info[name] = {
            'type': layer_type,
            'shapes': new_module.get_lora_param_shapes(),
            'param_count': new_module.lora_param_count
        }

    return model, lora_layers_info


def collect_lora_layers(model: nn.Module) -> Dict[str, Union[LoRALinear, LoRAConv1d]]:
    """
    Collect all LoRA layers from a model.

    Args:
        model: Model with injected LoRA layers

    Returns:
        Dict mapping layer names to LoRA layer modules
    """
    lora_layers = {}
    for name, module in model.named_modules():
        if isinstance(module, (LoRALinear, LoRAConv1d, LoRAEmbedding)):
            lora_layers[name] = module
    return lora_layers


def set_all_lora_params(
    model: nn.Module,
    lora_params: Dict[str, Tuple[torch.Tensor, torch.Tensor]]
):
    """
    Set LoRA parameters for all LoRA layers in the model.

    Args:
        model: Model with LoRA layers
        lora_params: Dict mapping layer names to (A, B) parameter tuples
    """
    lora_layers = collect_lora_layers(model)
    for name, (A, B) in lora_params.items():
        if name in lora_layers:
            lora_layers[name].set_lora_params(A, B)


def clear_all_lora_params(model: nn.Module):
    """
    Clear LoRA parameters from all LoRA layers in the model.

    Args:
        model: Model with LoRA layers
    """
    for module in model.modules():
        if isinstance(module, (LoRALinear, LoRAConv1d, LoRAEmbedding)):
            module.clear_lora_params()


def get_total_lora_param_count(model: nn.Module, rank: int = 8) -> int:
    """
    Get total number of LoRA parameters needed for the model.

    Args:
        model: Model to analyze
        rank: LoRA rank

    Returns:
        Total parameter count
    """
    dims = get_lora_param_dims(model, rank)
    return sum(d['total'] for d in dims.values())
