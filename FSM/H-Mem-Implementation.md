# H-Mem Implementation Guide

**Date:** 2025-12-13
**Version:** 1.0
**Author:** Claude Code Assistant
**Status:** Implementation Ready

---

## 1. Executive Summary

本文档提供 H-Mem (Horizon-Bridging Neural Memory Network) 在 OnlineTSF 框架中的完整实现方案。H-Mem 通过**双重记忆机制**（参数化神经记忆 + 非参数误差记忆库）解决在线时序预测中的反馈延迟问题。

### 核心设计决策

| 决策点 | 选择 | 理由 |
|--------|------|------|
| 适配器类型 | LoRA (Low-Rank Adaptation) | 比 SSF/Down-Up 更适合动态权重生成，参数效率更高 |
| 记忆更新 | 梯度驱动 + 惊奇度门控 | 兼容 Titans 理念，避免噪声累积 |
| 检索算法 | Cosine Similarity + Top-K | 简单高效，可后期替换为 FAISS |
| 融合机制 | 学习门控 (Learned Gating) | 避免简单加权的冲突问题 |

---

## 2. Architecture Overview

```
┌──────────────────────────────────────────────────────────────────────────┐
│                           H-Mem Framework                                │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│  ┌────────────────────────────────────────────────────────────────────┐  │
│  │                     Frozen Backbone                                │  │
│  │  (PatchTST / iTransformer / etc. with LoRA injection points)      │  │
│  │                                                                    │  │
│  │  Linear → LoRALinear(frozen_weight, trainable_A, trainable_B)     │  │
│  └────────────────────────────────────────────────────────────────────┘  │
│                              ↑                                           │
│                    LoRA params (ΔW = A·B)                                │
│                              ↑                                           │
│  ┌────────────────────────────────────────────────────────────────────┐  │
│  │              HyperNetwork (LoRA Generator)                         │  │
│  │  ┌──────────────┐    ┌─────────────┐    ┌──────────────────────┐  │  │
│  │  │ Memory State │ →  │  Bottleneck │ →  │ Per-Layer LoRA Params│  │  │
│  │  │     M_t      │    │   (r=8~32)  │    │   {A_l, B_l}_{l=1}^L │  │  │
│  │  └──────────────┘    └─────────────┘    └──────────────────────────┘  │  │
│  └────────────────────────────────────────────────────────────────────┘  │
│                              ↑                                           │
│              ┌───────────────┴───────────────┐                           │
│              ↓                               ↓                           │
│  ┌──────────────────────┐      ┌──────────────────────────────────────┐  │
│  │  SNMA Module         │      │  CHRC Module                         │  │
│  │  (Short-term Neural  │      │  (Cross-Horizon Retrieval Corrector) │  │
│  │   Memory Adapter)    │      │                                      │  │
│  │                      │      │  ┌────────────────────────────────┐  │  │
│  │  Input: POGT         │      │  │  Error Memory Bank             │  │  │
│  │  Output: M_t update  │      │  │  {Key: POGT_feat, Val: E_full} │  │  │
│  │                      │      │  └────────────────────────────────┘  │  │
│  │  • Surprise calc     │      │                                      │  │
│  │  • Gated memory upd  │      │  • Query: current POGT feature       │  │
│  │  • Forget mechanism  │      │  • Retrieve: Top-K similar           │  │
│  └──────────────────────┘      │  • Aggregate: Weighted residuals     │  │
│                                └──────────────────────────────────────┘  │
│                                               ↓                          │
│  ┌────────────────────────────────────────────────────────────────────┐  │
│  │                    Fusion Gate                                     │  │
│  │  Y_final = Y_adapt + λ(context) · E_retrieved                     │  │
│  │  where λ = sigmoid(MLP([Y_adapt; E_retrieved; POGT_feat]))        │  │
│  └────────────────────────────────────────────────────────────────────┘  │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

---

## 3. File Structure

```
OnlineTSF-main/
├── adapter/
│   ├── module/
│   │   ├── base.py              # 现有：基础适配接口
│   │   ├── lora.py              # 新增：LoRA 层实现
│   │   ├── neural_memory.py     # 新增：神经记忆模块
│   │   └── ...
│   ├── proceed.py               # 现有：PROCEED 实现
│   └── hmem.py                  # 新增：H-Mem 主模块
│
├── exp/
│   ├── exp_online.py            # 现有：在线学习基类
│   ├── exp_proceed.py           # 现有：PROCEED 实验类
│   └── exp_hmem.py              # 新增：H-Mem 实验类
│
├── util/
│   ├── buffer.py                # 现有：经验回放缓冲区
│   └── error_bank.py            # 新增：误差记忆库
│
├── settings.py                  # 修改：添加 H-Mem 配置
└── run.py                       # 修改：添加 H-Mem 入口
```

---

## 4. Detailed Implementation

### 4.1 LoRA Layer Implementation

**File:** `adapter/module/lora.py`

```python
"""
LoRA (Low-Rank Adaptation) layers for H-Mem.

Key difference from standard LoRA:
- Parameters A, B are GENERATED by HyperNetwork, not directly learned
- Supports dynamic injection at each forward pass
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Optional, Tuple


class LoRALinear(nn.Module):
    """
    Linear layer with LoRA adaptation capability.

    Forward: y = x @ (W + A @ B)^T + bias
           = x @ W^T + x @ (A @ B)^T + bias
           = original_output + lora_delta

    Attributes:
        in_features: Input dimension
        out_features: Output dimension
        rank: LoRA rank (r)
        weight: Frozen original weight [out_features, in_features]
        bias: Optional frozen bias [out_features]
        lora_A: Dynamic LoRA A matrix [rank, in_features]
        lora_B: Dynamic LoRA B matrix [out_features, rank]
        scaling: Scaling factor (alpha / rank)
    """

    def __init__(
        self,
        original_layer: nn.Linear,
        rank: int = 8,
        alpha: float = 16.0,
        dropout: float = 0.0
    ):
        super().__init__()

        self.in_features = original_layer.in_features
        self.out_features = original_layer.out_features
        self.rank = rank
        self.scaling = alpha / rank

        # Freeze original weights
        self.register_buffer('weight', original_layer.weight.data.clone())
        if original_layer.bias is not None:
            self.register_buffer('bias', original_layer.bias.data.clone())
        else:
            self.register_buffer('bias', None)

        # LoRA parameters (will be set dynamically)
        self.lora_A: Optional[torch.Tensor] = None  # [rank, in_features]
        self.lora_B: Optional[torch.Tensor] = None  # [out_features, rank]

        # Dropout for regularization
        self.dropout = nn.Dropout(dropout) if dropout > 0 else nn.Identity()

        # For computing parameter dimensions
        self.lora_param_count = rank * (self.in_features + self.out_features)

    def set_lora_params(self, lora_A: torch.Tensor, lora_B: torch.Tensor):
        """
        Inject LoRA parameters from HyperNetwork.

        Args:
            lora_A: [batch, rank, in_features] or [rank, in_features]
            lora_B: [batch, out_features, rank] or [out_features, rank]
        """
        self.lora_A = lora_A
        self.lora_B = lora_B

    def clear_lora_params(self):
        """Reset LoRA parameters (for inference without adaptation)."""
        self.lora_A = None
        self.lora_B = None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Forward pass with optional LoRA adaptation.

        Args:
            x: Input tensor [batch, ..., in_features]

        Returns:
            Output tensor [batch, ..., out_features]
        """
        # Original linear transformation
        result = F.linear(x, self.weight, self.bias)

        # Apply LoRA delta if parameters are set
        if self.lora_A is not None and self.lora_B is not None:
            x_dropped = self.dropout(x)

            # Handle batched LoRA parameters
            if self.lora_A.dim() == 3:  # [batch, rank, in_features]
                # Per-sample adaptation
                # x: [batch, seq, in_features]
                # lora_A: [batch, rank, in_features]
                # lora_B: [batch, out_features, rank]
                batch_size = x.size(0)

                # Compute x @ A^T: [batch, seq, rank]
                lora_output = torch.bmm(
                    x_dropped.view(batch_size, -1, self.in_features),
                    self.lora_A.transpose(-2, -1)
                )

                # Compute (x @ A^T) @ B^T: [batch, seq, out_features]
                lora_output = torch.bmm(lora_output, self.lora_B.transpose(-2, -1))
                lora_output = lora_output.view_as(result)

            else:  # [rank, in_features] - shared across batch
                # x @ A^T @ B^T
                lora_output = F.linear(F.linear(x_dropped, self.lora_A), self.lora_B)

            result = result + self.scaling * lora_output

        return result

    def extra_repr(self) -> str:
        return (f'in_features={self.in_features}, out_features={self.out_features}, '
                f'rank={self.rank}, scaling={self.scaling:.2f}')


class LoRAConv1d(nn.Module):
    """
    Conv1d layer with LoRA adaptation (for Transformer's Conv projections).

    Treats convolution as matrix multiplication for LoRA application.
    """

    def __init__(
        self,
        original_layer: nn.Conv1d,
        rank: int = 8,
        alpha: float = 16.0
    ):
        super().__init__()

        self.in_channels = original_layer.in_channels
        self.out_channels = original_layer.out_channels
        self.kernel_size = original_layer.kernel_size[0]
        self.rank = rank
        self.scaling = alpha / rank

        # Only support kernel_size=1 for simplicity (common in Transformers)
        assert self.kernel_size == 1, "LoRAConv1d only supports kernel_size=1"

        # Freeze original weights
        self.register_buffer('weight', original_layer.weight.data.clone())
        if original_layer.bias is not None:
            self.register_buffer('bias', original_layer.bias.data.clone())
        else:
            self.register_buffer('bias', None)

        self.lora_A: Optional[torch.Tensor] = None
        self.lora_B: Optional[torch.Tensor] = None

        self.lora_param_count = rank * (self.in_channels + self.out_channels)

    def set_lora_params(self, lora_A: torch.Tensor, lora_B: torch.Tensor):
        self.lora_A = lora_A
        self.lora_B = lora_B

    def clear_lora_params(self):
        self.lora_A = None
        self.lora_B = None

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """
        Args:
            x: [batch, in_channels, length]
        Returns:
            [batch, out_channels, length]
        """
        result = F.conv1d(x, self.weight, self.bias)

        if self.lora_A is not None and self.lora_B is not None:
            # Reshape to [batch, length, in_channels] for linear ops
            x_t = x.transpose(1, 2)

            if self.lora_A.dim() == 3:
                batch_size = x.size(0)
                lora_out = torch.bmm(
                    x_t.reshape(batch_size, -1, self.in_channels),
                    self.lora_A.transpose(-2, -1)
                )
                lora_out = torch.bmm(lora_out, self.lora_B.transpose(-2, -1))
                lora_out = lora_out.view(batch_size, -1, self.out_channels)
            else:
                lora_out = F.linear(F.linear(x_t, self.lora_A), self.lora_B)

            # Reshape back to [batch, out_channels, length]
            result = result + self.scaling * lora_out.transpose(1, 2)

        return result


def get_lora_param_dims(model: nn.Module, rank: int = 8) -> dict:
    """
    Calculate LoRA parameter dimensions for each layer in the model.

    Returns:
        Dict mapping layer names to (A_shape, B_shape) tuples
    """
    dims = {}
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            dims[name] = {
                'A': (rank, module.in_features),
                'B': (module.out_features, rank),
                'total': rank * (module.in_features + module.out_features)
            }
        elif isinstance(module, nn.Conv1d) and module.kernel_size[0] == 1:
            dims[name] = {
                'A': (rank, module.in_channels),
                'B': (module.out_channels, rank),
                'total': rank * (module.in_channels + module.out_channels)
            }
    return dims


def inject_lora_layers(
    model: nn.Module,
    rank: int = 8,
    alpha: float = 16.0,
    target_modules: Optional[list] = None
) -> Tuple[nn.Module, list]:
    """
    Replace Linear/Conv1d layers with LoRA-enabled versions.

    Args:
        model: Original model
        rank: LoRA rank
        alpha: LoRA scaling factor
        target_modules: List of module name patterns to inject (None = all)

    Returns:
        Modified model, list of LoRA layer names
    """
    lora_layers = []

    def should_replace(name: str) -> bool:
        if target_modules is None:
            return True
        return any(pattern in name for pattern in target_modules)

    def replace_module(parent: nn.Module, name: str, module: nn.Module):
        if isinstance(module, nn.Linear) and should_replace(name):
            lora_layer = LoRALinear(module, rank=rank, alpha=alpha)
            setattr(parent, name.split('.')[-1], lora_layer)
            lora_layers.append(name)
        elif isinstance(module, nn.Conv1d) and module.kernel_size[0] == 1 and should_replace(name):
            lora_layer = LoRAConv1d(module, rank=rank, alpha=alpha)
            setattr(parent, name.split('.')[-1], lora_layer)
            lora_layers.append(name)

    # Traverse and replace
    for name, module in list(model.named_modules()):
        if '.' in name:
            parent_name = '.'.join(name.split('.')[:-1])
            parent = dict(model.named_modules())[parent_name]
            replace_module(parent, name, module)
        elif name:
            replace_module(model, name, module)

    return model, lora_layers
```

---

### 4.2 Neural Memory Module

**File:** `adapter/module/neural_memory.py`

```python
"""
Short-term Neural Memory Adapter (SNMA) for H-Mem.

Inspired by Titans' neural long-term memory, adapted for online TSF.
Key innovations:
- Surprise-gated updates to filter noise
- Momentum-based smooth transitions
- Forget mechanism for old patterns
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple


class SurpriseCalculator(nn.Module):
    """
    Calculates "surprise" as the gradient magnitude of POGT prediction error.
    High surprise → large memory update; Low surprise → small update.
    """

    def __init__(self, input_dim: int, hidden_dim: int = 64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LayerNorm(hidden_dim),
            nn.GELU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        self.predictor = nn.Linear(hidden_dim, input_dim)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Args:
            x: POGT values [batch, pogt_len, features]

        Returns:
            surprise: Scalar surprise score [batch]
            encoding: Encoded representation [batch, hidden_dim]
        """
        # Encode the sequence
        h = self.encoder(x)  # [batch, pogt_len, hidden_dim]
        h_mean = h.mean(dim=1)  # [batch, hidden_dim]

        # Predict next step (autoregressive surprise)
        pred = self.predictor(h[:, :-1])  # [batch, pogt_len-1, features]
        target = x[:, 1:]  # [batch, pogt_len-1, features]

        # Surprise = prediction error magnitude
        error = (pred - target).pow(2).mean(dim=(1, 2))  # [batch]
        surprise = torch.sqrt(error + 1e-8)

        return surprise, h_mean


class NeuralMemoryState(nn.Module):
    """
    Maintains and updates the neural memory state M_t.

    Memory update rule (Titans-inspired):
    M_t = (1 - forget_gate) * M_{t-1} + surprise_gate * new_info

    Where:
    - forget_gate: Learned forgetting based on time and content
    - surprise_gate: Gates update based on novelty
    """

    def __init__(
        self,
        memory_dim: int = 256,
        input_dim: int = 64,
        num_heads: int = 4,
        momentum: float = 0.9
    ):
        super().__init__()

        self.memory_dim = memory_dim
        self.input_dim = input_dim
        self.num_heads = num_heads
        self.momentum = momentum

        # Memory state (registered as buffer for persistence)
        self.register_buffer('memory', torch.zeros(1, memory_dim))
        self.register_buffer('memory_age', torch.zeros(1))

        # Input projection
        self.input_proj = nn.Linear(input_dim, memory_dim)

        # Forget gate: decides how much of old memory to retain
        self.forget_gate = nn.Sequential(
            nn.Linear(memory_dim + input_dim, memory_dim),
            nn.Sigmoid()
        )

        # Update gate: decides how much new info to incorporate
        self.update_gate = nn.Sequential(
            nn.Linear(memory_dim + input_dim + 1, memory_dim),  # +1 for surprise
            nn.Sigmoid()
        )

        # Content generator: transforms input to memory content
        self.content_gen = nn.Sequential(
            nn.Linear(input_dim, memory_dim),
            nn.LayerNorm(memory_dim),
            nn.GELU(),
            nn.Linear(memory_dim, memory_dim),
            nn.Tanh()
        )

        # Multi-head attention for memory read
        self.memory_attention = nn.MultiheadAttention(
            embed_dim=memory_dim,
            num_heads=num_heads,
            batch_first=True
        )

    def reset_memory(self, batch_size: int = 1):
        """Reset memory state to initial values."""
        device = self.memory.device
        self.memory = torch.zeros(batch_size, self.memory_dim, device=device)
        self.memory_age = torch.zeros(batch_size, device=device)

    def update(
        self,
        new_info: torch.Tensor,
        surprise: torch.Tensor
    ) -> torch.Tensor:
        """
        Update memory state with new information.

        Args:
            new_info: Encoded POGT information [batch, input_dim]
            surprise: Surprise scores [batch]

        Returns:
            Updated memory state [batch, memory_dim]
        """
        batch_size = new_info.size(0)

        # Expand memory if batch size changed
        if self.memory.size(0) != batch_size:
            self.memory = self.memory.expand(batch_size, -1).clone()
            self.memory_age = self.memory_age.expand(batch_size).clone()

        # Current memory
        M_prev = self.memory

        # Compute forget gate
        forget_input = torch.cat([M_prev, new_info], dim=-1)
        f_t = self.forget_gate(forget_input)

        # Compute update gate (surprise-modulated)
        surprise_expanded = surprise.unsqueeze(-1)  # [batch, 1]
        update_input = torch.cat([M_prev, new_info, surprise_expanded], dim=-1)
        u_t = self.update_gate(update_input)

        # Generate new content
        c_t = self.content_gen(new_info)

        # Update memory: M_t = f_t * M_{t-1} + u_t * c_t
        # Apply momentum for stability
        M_new_raw = f_t * M_prev + u_t * c_t
        M_new = self.momentum * M_prev + (1 - self.momentum) * M_new_raw

        # Update state
        self.memory = M_new
        self.memory_age = self.memory_age + 1

        return M_new

    def read(self, query: Optional[torch.Tensor] = None) -> torch.Tensor:
        """
        Read from memory state.

        Args:
            query: Optional query for attention-based read [batch, query_dim]

        Returns:
            Memory representation [batch, memory_dim]
        """
        if query is None:
            return self.memory

        # Attention-based read
        memory_expanded = self.memory.unsqueeze(1)  # [batch, 1, memory_dim]
        query_expanded = query.unsqueeze(1)  # [batch, 1, query_dim]

        # Project query to memory_dim if needed
        if query.size(-1) != self.memory_dim:
            query_expanded = F.linear(
                query_expanded,
                torch.eye(self.memory_dim, query.size(-1), device=query.device)
            )

        attended, _ = self.memory_attention(
            query_expanded, memory_expanded, memory_expanded
        )

        return attended.squeeze(1)


class SNMA(nn.Module):
    """
    Short-term Neural Memory Adapter.

    Complete module combining:
    - Surprise calculation from POGT
    - Memory state update
    - LoRA parameter generation via HyperNetwork
    """

    def __init__(
        self,
        input_features: int,
        memory_dim: int = 256,
        lora_total_params: int = 10000,
        bottleneck_dim: int = 32,
        momentum: float = 0.9
    ):
        super().__init__()

        self.input_features = input_features
        self.memory_dim = memory_dim
        self.bottleneck_dim = bottleneck_dim

        # Surprise calculator
        self.surprise_calc = SurpriseCalculator(
            input_dim=input_features,
            hidden_dim=memory_dim // 2
        )

        # Neural memory state
        self.memory = NeuralMemoryState(
            memory_dim=memory_dim,
            input_dim=memory_dim // 2,
            momentum=momentum
        )

        # HyperNetwork: Memory → LoRA parameters
        self.hypernet = nn.Sequential(
            nn.Linear(memory_dim, bottleneck_dim),
            nn.LayerNorm(bottleneck_dim),
            nn.GELU(),
            nn.Linear(bottleneck_dim, bottleneck_dim),
            nn.GELU(),
            nn.Linear(bottleneck_dim, lora_total_params)
        )

        # Layer-wise parameter splitter (set during initialization)
        self.layer_param_dims: dict = {}

    def register_lora_layers(self, layer_dims: dict):
        """
        Register LoRA layer dimensions for parameter splitting.

        Args:
            layer_dims: Dict from get_lora_param_dims()
        """
        self.layer_param_dims = layer_dims
        total_params = sum(d['total'] for d in layer_dims.values())

        # Reinitialize hypernet output if needed
        if self.hypernet[-1].out_features != total_params:
            self.hypernet[-1] = nn.Linear(
                self.bottleneck_dim, total_params
            ).to(next(self.parameters()).device)

    def forward(
        self,
        pogt: torch.Tensor
    ) -> Tuple[dict, torch.Tensor]:
        """
        Process POGT and generate LoRA parameters.

        Args:
            pogt: Partially Observed Ground Truth [batch, pogt_len, features]

        Returns:
            lora_params: Dict mapping layer names to (A, B) tuples
            memory_state: Current memory state for debugging/analysis
        """
        # Calculate surprise
        surprise, encoding = self.surprise_calc(pogt)

        # Update memory
        memory_state = self.memory.update(encoding, surprise)

        # Generate LoRA parameters
        flat_params = self.hypernet(memory_state)  # [batch, total_params]

        # Split into per-layer parameters
        lora_params = {}
        offset = 0
        for name, dims in self.layer_param_dims.items():
            a_size = dims['A'][0] * dims['A'][1]  # rank * in_features
            b_size = dims['B'][0] * dims['B'][1]  # out_features * rank

            a_flat = flat_params[:, offset:offset + a_size]
            b_flat = flat_params[:, offset + a_size:offset + a_size + b_size]

            A = a_flat.view(-1, dims['A'][0], dims['A'][1])
            B = b_flat.view(-1, dims['B'][0], dims['B'][1])

            lora_params[name] = (A, B)
            offset += a_size + b_size

        return lora_params, memory_state

    def reset(self, batch_size: int = 1):
        """Reset memory state for new sequence."""
        self.memory.reset_memory(batch_size)
```

---

### 4.3 Error Memory Bank

**File:** `util/error_bank.py`

```python
"""
Error Memory Bank for Cross-Horizon Retrieval Corrector (CHRC).

Stores historical {POGT feature → Full horizon error} mappings
for retrieval-augmented error correction.

Design considerations:
- Fixed capacity with intelligent eviction (LRU + importance-based)
- Efficient retrieval using cosine similarity
- Temporal decay for old entries
- Optional clustering for memory efficiency
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, List
from collections import OrderedDict
import numpy as np


class ErrorMemoryBank(nn.Module):
    """
    Non-parametric memory bank storing POGT-Error pairs.

    Attributes:
        capacity: Maximum number of entries
        feature_dim: Dimension of POGT features
        horizon: Prediction horizon (error length)
        num_features: Number of time series features
    """

    def __init__(
        self,
        capacity: int = 1000,
        feature_dim: int = 128,
        horizon: int = 24,
        num_features: int = 7,
        decay_factor: float = 0.99,
        temperature: float = 0.1
    ):
        super().__init__()

        self.capacity = capacity
        self.feature_dim = feature_dim
        self.horizon = horizon
        self.num_features = num_features
        self.decay_factor = decay_factor
        self.temperature = temperature

        # Storage buffers
        self.register_buffer('keys', torch.zeros(capacity, feature_dim))
        self.register_buffer('values', torch.zeros(capacity, horizon, num_features))
        self.register_buffer('timestamps', torch.zeros(capacity, dtype=torch.long))
        self.register_buffer('access_counts', torch.zeros(capacity, dtype=torch.long))
        self.register_buffer('importance_scores', torch.zeros(capacity))

        # Pointer and count
        self.register_buffer('current_idx', torch.tensor(0, dtype=torch.long))
        self.register_buffer('total_entries', torch.tensor(0, dtype=torch.long))
        self.register_buffer('global_time', torch.tensor(0, dtype=torch.long))

        # Feature encoder for POGT
        self.feature_encoder = nn.Sequential(
            nn.Linear(horizon * num_features, feature_dim * 2),
            nn.LayerNorm(feature_dim * 2),
            nn.GELU(),
            nn.Linear(feature_dim * 2, feature_dim),
            nn.LayerNorm(feature_dim)
        )

    def encode_pogt(self, pogt: torch.Tensor) -> torch.Tensor:
        """
        Encode POGT into feature vector for retrieval.

        Args:
            pogt: [batch, pogt_len, features] or [batch, features]

        Returns:
            Encoded features [batch, feature_dim]
        """
        if pogt.dim() == 3:
            pogt_flat = pogt.reshape(pogt.size(0), -1)
        else:
            pogt_flat = pogt

        # Pad or truncate to expected size
        expected_size = self.horizon * self.num_features
        if pogt_flat.size(-1) < expected_size:
            pogt_flat = F.pad(pogt_flat, (0, expected_size - pogt_flat.size(-1)))
        elif pogt_flat.size(-1) > expected_size:
            pogt_flat = pogt_flat[..., :expected_size]

        return self.feature_encoder(pogt_flat)

    def store(
        self,
        pogt_features: torch.Tensor,
        errors: torch.Tensor,
        importance: Optional[torch.Tensor] = None
    ):
        """
        Store new POGT-Error pairs in memory.

        Args:
            pogt_features: Encoded POGT features [batch, feature_dim]
            errors: Full horizon errors [batch, horizon, features]
            importance: Optional importance scores [batch]
        """
        batch_size = pogt_features.size(0)

        # Default importance based on error magnitude
        if importance is None:
            importance = errors.abs().mean(dim=(1, 2))

        for i in range(batch_size):
            # Find insertion index
            idx = self._get_insertion_idx()

            # Store entry
            self.keys[idx] = pogt_features[i].detach()
            self.values[idx] = errors[i].detach()
            self.timestamps[idx] = self.global_time.clone()
            self.access_counts[idx] = 0
            self.importance_scores[idx] = importance[i].detach()

            # Update pointers
            self.current_idx = (self.current_idx + 1) % self.capacity
            self.total_entries = min(self.total_entries + 1, self.capacity)

        self.global_time += 1

    def _get_insertion_idx(self) -> int:
        """
        Get index for new entry, using intelligent eviction if at capacity.
        """
        if self.total_entries < self.capacity:
            return int(self.current_idx.item())

        # Eviction score: low importance + old + rarely accessed = evict
        time_factor = (self.global_time - self.timestamps).float()
        time_factor = time_factor / (time_factor.max() + 1e-8)

        access_factor = self.access_counts.float()
        access_factor = access_factor / (access_factor.max() + 1e-8)

        importance_factor = self.importance_scores
        importance_factor = importance_factor / (importance_factor.max() + 1e-8)

        # Lower score = more likely to evict
        eviction_score = (
            importance_factor * 0.4 +
            access_factor * 0.3 +
            (1 - time_factor) * 0.3  # Prefer keeping recent entries
        )

        return int(eviction_score.argmin().item())

    def retrieve(
        self,
        query_features: torch.Tensor,
        top_k: int = 5,
        min_similarity: float = 0.3
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Retrieve top-K similar error patterns.

        Args:
            query_features: Query POGT features [batch, feature_dim]
            top_k: Number of entries to retrieve
            min_similarity: Minimum similarity threshold

        Returns:
            retrieved_errors: [batch, top_k, horizon, features]
            similarities: [batch, top_k]
            valid_mask: [batch, top_k] boolean mask for valid retrievals
        """
        batch_size = query_features.size(0)
        n_entries = int(self.total_entries.item())

        if n_entries == 0:
            # Empty memory bank
            return (
                torch.zeros(batch_size, top_k, self.horizon, self.num_features,
                           device=query_features.device),
                torch.zeros(batch_size, top_k, device=query_features.device),
                torch.zeros(batch_size, top_k, dtype=torch.bool, device=query_features.device)
            )

        # Get valid keys
        valid_keys = self.keys[:n_entries]  # [n_entries, feature_dim]
        valid_values = self.values[:n_entries]  # [n_entries, horizon, features]

        # Compute cosine similarity
        query_norm = F.normalize(query_features, p=2, dim=-1)  # [batch, feature_dim]
        keys_norm = F.normalize(valid_keys, p=2, dim=-1)  # [n_entries, feature_dim]

        similarities = torch.mm(query_norm, keys_norm.t())  # [batch, n_entries]

        # Apply temporal decay
        time_diff = self.global_time - self.timestamps[:n_entries]
        decay = torch.pow(self.decay_factor, time_diff.float())
        similarities = similarities * decay.unsqueeze(0)

        # Get top-K
        actual_k = min(top_k, n_entries)
        top_sims, top_indices = similarities.topk(actual_k, dim=-1)

        # Gather retrieved errors
        retrieved_errors = valid_values[top_indices]  # [batch, actual_k, horizon, features]

        # Create validity mask
        valid_mask = top_sims >= min_similarity

        # Pad if needed
        if actual_k < top_k:
            pad_size = top_k - actual_k
            retrieved_errors = F.pad(retrieved_errors, (0, 0, 0, 0, 0, pad_size))
            top_sims = F.pad(top_sims, (0, pad_size))
            valid_mask = F.pad(valid_mask, (0, pad_size), value=False)

        # Update access counts
        for batch_idx in range(batch_size):
            for k in range(actual_k):
                if valid_mask[batch_idx, k]:
                    entry_idx = top_indices[batch_idx, k]
                    self.access_counts[entry_idx] += 1

        return retrieved_errors, top_sims, valid_mask

    def aggregate(
        self,
        retrieved_errors: torch.Tensor,
        similarities: torch.Tensor,
        valid_mask: torch.Tensor,
        aggregation: str = 'weighted_mean'
    ) -> torch.Tensor:
        """
        Aggregate retrieved errors into a single correction term.

        Args:
            retrieved_errors: [batch, top_k, horizon, features]
            similarities: [batch, top_k]
            valid_mask: [batch, top_k]
            aggregation: 'weighted_mean', 'attention', or 'max'

        Returns:
            Aggregated error correction [batch, horizon, features]
        """
        batch_size = retrieved_errors.size(0)

        # Mask invalid entries
        masked_sims = similarities * valid_mask.float()

        if aggregation == 'weighted_mean':
            # Softmax weights
            weights = F.softmax(masked_sims / self.temperature, dim=-1)
            weights = weights * valid_mask.float()
            weights = weights / (weights.sum(dim=-1, keepdim=True) + 1e-8)

            # Weighted sum
            aggregated = torch.einsum('bk,bkhf->bhf', weights, retrieved_errors)

        elif aggregation == 'max':
            # Take the most similar entry
            max_indices = masked_sims.argmax(dim=-1)
            aggregated = retrieved_errors[torch.arange(batch_size), max_indices]

        else:  # attention
            # Use similarities as attention scores
            attn = F.softmax(masked_sims / self.temperature, dim=-1).unsqueeze(-1).unsqueeze(-1)
            aggregated = (attn * retrieved_errors).sum(dim=1)

        # Zero out if no valid retrievals
        no_valid = ~valid_mask.any(dim=-1, keepdim=True)
        aggregated = aggregated * (~no_valid).float().unsqueeze(-1)

        return aggregated

    def get_stats(self) -> dict:
        """Get memory bank statistics."""
        n_entries = int(self.total_entries.item())
        return {
            'total_entries': n_entries,
            'capacity': self.capacity,
            'utilization': n_entries / self.capacity,
            'avg_access_count': self.access_counts[:n_entries].float().mean().item() if n_entries > 0 else 0,
            'avg_importance': self.importance_scores[:n_entries].mean().item() if n_entries > 0 else 0,
            'avg_age': (self.global_time - self.timestamps[:n_entries]).float().mean().item() if n_entries > 0 else 0
        }

    def clear(self):
        """Clear all entries from memory bank."""
        self.keys.zero_()
        self.values.zero_()
        self.timestamps.zero_()
        self.access_counts.zero_()
        self.importance_scores.zero_()
        self.current_idx.zero_()
        self.total_entries.zero_()


class CHRC(nn.Module):
    """
    Cross-Horizon Retrieval Corrector.

    Complete module combining:
    - POGT feature encoding
    - Historical error retrieval
    - Adaptive error aggregation
    - Confidence-gated correction
    """

    def __init__(
        self,
        pogt_len: int,
        horizon: int,
        num_features: int,
        feature_dim: int = 128,
        capacity: int = 1000,
        top_k: int = 5,
        temperature: float = 0.1
    ):
        super().__init__()

        self.pogt_len = pogt_len
        self.horizon = horizon
        self.num_features = num_features
        self.top_k = top_k

        # Error memory bank
        self.memory_bank = ErrorMemoryBank(
            capacity=capacity,
            feature_dim=feature_dim,
            horizon=horizon,
            num_features=num_features,
            temperature=temperature
        )

        # Confidence gate: decides how much to trust retrieval
        # Input: [prediction, retrieved_correction, pogt_features]
        gate_input_dim = horizon * num_features * 2 + feature_dim
        self.confidence_gate = nn.Sequential(
            nn.Linear(gate_input_dim, feature_dim),
            nn.LayerNorm(feature_dim),
            nn.GELU(),
            nn.Linear(feature_dim, 1),
            nn.Sigmoid()
        )

        # Refinement network: refines retrieved correction
        self.refiner = nn.Sequential(
            nn.Linear(horizon * num_features, feature_dim),
            nn.GELU(),
            nn.Linear(feature_dim, horizon * num_features)
        )

    def forward(
        self,
        prediction: torch.Tensor,
        pogt: torch.Tensor,
        return_confidence: bool = False
    ) -> Tuple[torch.Tensor, ...]:
        """
        Apply retrieval-based correction to prediction.

        Args:
            prediction: Model prediction [batch, horizon, features]
            pogt: Partially observed ground truth [batch, pogt_len, features]
            return_confidence: Whether to return confidence score

        Returns:
            corrected_prediction: [batch, horizon, features]
            confidence: (optional) [batch, 1]
        """
        batch_size = prediction.size(0)

        # Encode POGT
        pogt_features = self.memory_bank.encode_pogt(pogt)  # [batch, feature_dim]

        # Retrieve similar error patterns
        retrieved_errors, similarities, valid_mask = self.memory_bank.retrieve(
            pogt_features, top_k=self.top_k
        )

        # Aggregate retrieved errors
        error_correction = self.memory_bank.aggregate(
            retrieved_errors, similarities, valid_mask,
            aggregation='weighted_mean'
        )  # [batch, horizon, features]

        # Refine correction
        correction_flat = error_correction.reshape(batch_size, -1)
        refined_correction = self.refiner(correction_flat)
        refined_correction = refined_correction.reshape(batch_size, self.horizon, self.num_features)

        # Compute confidence
        pred_flat = prediction.reshape(batch_size, -1)
        correction_flat = refined_correction.reshape(batch_size, -1)
        gate_input = torch.cat([pred_flat, correction_flat, pogt_features], dim=-1)
        confidence = self.confidence_gate(gate_input)  # [batch, 1]

        # Apply gated correction
        # When retrieval is unreliable (low similarity), confidence should be low
        has_valid = valid_mask.any(dim=-1, keepdim=True).float()
        effective_confidence = confidence * has_valid

        corrected = prediction + effective_confidence.unsqueeze(-1) * refined_correction

        if return_confidence:
            return corrected, confidence
        return corrected

    def store_error(
        self,
        pogt: torch.Tensor,
        actual_error: torch.Tensor
    ):
        """
        Store observed error for future retrieval.

        Called when full ground truth becomes available.

        Args:
            pogt: POGT at prediction time [batch, pogt_len, features]
            actual_error: Full horizon error [batch, horizon, features]
        """
        pogt_features = self.memory_bank.encode_pogt(pogt)
        self.memory_bank.store(pogt_features, actual_error)
```

---

### 4.4 H-Mem Main Module

**File:** `adapter/hmem.py`

```python
"""
H-Mem: Horizon-Bridging Neural Memory Network for Online Time Series Forecasting.

Main module integrating:
- Frozen backbone with LoRA injection
- SNMA (Short-term Neural Memory Adapter)
- CHRC (Cross-Horizon Retrieval Corrector)
- Fusion gate for combining adaptations
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Tuple, Dict, Any

from adapter.module.lora import inject_lora_layers, LoRALinear, LoRAConv1d, get_lora_param_dims
from adapter.module.neural_memory import SNMA
from util.error_bank import CHRC


class HMem(nn.Module):
    """
    H-Mem: Horizon-Bridging Neural Memory Network.

    Architecture:
    1. Frozen Backbone with LoRA injection points
    2. SNMA: Generates LoRA params from POGT via neural memory
    3. CHRC: Retrieves historical error patterns for correction
    4. Fusion Gate: Combines adapted prediction with correction

    Args:
        backbone: Pre-trained forecasting model (PatchTST, iTransformer, etc.)
        args: Configuration arguments
    """

    def __init__(self, backbone: nn.Module, args):
        super().__init__()

        self.args = args
        self.seq_len = args.seq_len
        self.pred_len = args.pred_len
        self.enc_in = args.enc_in  # Number of features

        # H-Mem specific parameters
        self.lora_rank = getattr(args, 'lora_rank', 8)
        self.lora_alpha = getattr(args, 'lora_alpha', 16.0)
        self.memory_dim = getattr(args, 'memory_dim', 256)
        self.bottleneck_dim = getattr(args, 'bottleneck_dim', 32)
        self.memory_capacity = getattr(args, 'memory_capacity', 1000)
        self.retrieval_top_k = getattr(args, 'retrieval_top_k', 5)
        self.pogt_ratio = getattr(args, 'pogt_ratio', 0.5)  # Portion of horizon used as POGT

        # Calculate POGT length
        self.pogt_len = max(1, int(self.pred_len * self.pogt_ratio))

        # 1. Inject LoRA layers into backbone and freeze original weights
        self.backbone, self.lora_layer_names = inject_lora_layers(
            backbone,
            rank=self.lora_rank,
            alpha=self.lora_alpha,
            target_modules=getattr(args, 'lora_target_modules', None)
        )
        self._freeze_backbone()

        # Get LoRA parameter dimensions
        lora_dims = self._collect_lora_dims()
        total_lora_params = sum(d['total'] for d in lora_dims.values())

        # 2. SNMA: Short-term Neural Memory Adapter
        self.snma = SNMA(
            input_features=self.enc_in,
            memory_dim=self.memory_dim,
            lora_total_params=total_lora_params,
            bottleneck_dim=self.bottleneck_dim,
            momentum=getattr(args, 'memory_momentum', 0.9)
        )
        self.snma.register_lora_layers(lora_dims)

        # 3. CHRC: Cross-Horizon Retrieval Corrector
        self.chrc = CHRC(
            pogt_len=self.pogt_len,
            horizon=self.pred_len,
            num_features=self.enc_in,
            feature_dim=getattr(args, 'chrc_feature_dim', 128),
            capacity=self.memory_capacity,
            top_k=self.retrieval_top_k,
            temperature=getattr(args, 'chrc_temperature', 0.1)
        )

        # 4. Fusion Gate: Combines SNMA-adapted prediction with CHRC correction
        fusion_input_dim = self.pred_len * self.enc_in * 2 + self.memory_dim
        self.fusion_gate = nn.Sequential(
            nn.Linear(fusion_input_dim, self.memory_dim),
            nn.LayerNorm(self.memory_dim),
            nn.GELU(),
            nn.Linear(self.memory_dim, 1),
            nn.Sigmoid()
        )

        # State tracking
        self.register_buffer('_is_cold_start', torch.tensor(True))
        self._last_pogt: Optional[torch.Tensor] = None
        self._last_prediction: Optional[torch.Tensor] = None

        # For tracking trainable parameters
        self._init_trainable_params()

    def _freeze_backbone(self):
        """Freeze all backbone parameters except LoRA layers."""
        for name, param in self.backbone.named_parameters():
            # Keep LoRA-related parameters trainable if any
            # (But in our design, LoRA params are generated, not learned directly)
            param.requires_grad = False

    def _collect_lora_dims(self) -> Dict[str, dict]:
        """Collect LoRA parameter dimensions from injected layers."""
        dims = {}
        for name, module in self.backbone.named_modules():
            if isinstance(module, (LoRALinear, LoRAConv1d)):
                if isinstance(module, LoRALinear):
                    dims[name] = {
                        'A': (module.rank, module.in_features),
                        'B': (module.out_features, module.rank),
                        'total': module.lora_param_count
                    }
                else:
                    dims[name] = {
                        'A': (module.rank, module.in_channels),
                        'B': (module.out_channels, module.rank),
                        'total': module.lora_param_count
                    }
        return dims

    def _init_trainable_params(self):
        """Count trainable parameters for monitoring."""
        total = sum(p.numel() for p in self.parameters() if p.requires_grad)
        backbone_frozen = sum(p.numel() for p in self.backbone.parameters())
        self._param_stats = {
            'total_trainable': total,
            'backbone_frozen': backbone_frozen,
            'snma_params': sum(p.numel() for p in self.snma.parameters()),
            'chrc_params': sum(p.numel() for p in self.chrc.parameters()),
            'fusion_params': sum(p.numel() for p in self.fusion_gate.parameters())
        }

    def _inject_lora_params(self, lora_params: Dict[str, Tuple[torch.Tensor, torch.Tensor]]):
        """Inject generated LoRA parameters into backbone layers."""
        for name, module in self.backbone.named_modules():
            if name in lora_params:
                A, B = lora_params[name]
                module.set_lora_params(A, B)

    def _clear_lora_params(self):
        """Clear LoRA parameters from backbone layers."""
        for module in self.backbone.modules():
            if isinstance(module, (LoRALinear, LoRAConv1d)):
                module.clear_lora_params()

    def forward(
        self,
        x_enc: torch.Tensor,
        x_mark_enc: Optional[torch.Tensor] = None,
        pogt: Optional[torch.Tensor] = None,
        mode: str = 'predict'
    ) -> Dict[str, torch.Tensor]:
        """
        Forward pass with optional POGT for adaptation.

        Args:
            x_enc: Input sequence [batch, seq_len, features]
            x_mark_enc: Time features [batch, seq_len, time_features] (optional)
            pogt: Partially observed ground truth [batch, pogt_len, features] (optional)
            mode: 'predict' (inference) or 'adapt' (update memory)

        Returns:
            Dictionary containing:
            - 'prediction': Final prediction [batch, pred_len, features]
            - 'base_prediction': Backbone prediction without adaptation
            - 'adapted_prediction': SNMA-adapted prediction
            - 'correction': CHRC correction term
            - 'fusion_weight': Weight for correction
            - 'memory_state': Current memory state
        """
        batch_size = x_enc.size(0)
        outputs = {}

        # Step 1: Get base prediction (no LoRA)
        self._clear_lora_params()
        if x_mark_enc is not None:
            base_pred = self.backbone(x_enc, x_mark_enc)
        else:
            base_pred = self.backbone(x_enc)
        outputs['base_prediction'] = base_pred

        # If no POGT, return base prediction (cold start or inference without recent data)
        if pogt is None:
            outputs['prediction'] = base_pred
            outputs['adapted_prediction'] = base_pred
            outputs['correction'] = torch.zeros_like(base_pred)
            outputs['fusion_weight'] = torch.zeros(batch_size, 1, device=x_enc.device)
            return outputs

        # Step 2: SNMA - Generate LoRA params from POGT
        lora_params, memory_state = self.snma(pogt)
        outputs['memory_state'] = memory_state

        # Step 3: Get adapted prediction (with LoRA)
        self._inject_lora_params(lora_params)
        if x_mark_enc is not None:
            adapted_pred = self.backbone(x_enc, x_mark_enc)
        else:
            adapted_pred = self.backbone(x_enc)
        outputs['adapted_prediction'] = adapted_pred
        self._clear_lora_params()

        # Step 4: CHRC - Retrieve historical error patterns
        if self._is_cold_start:
            # Cold start: no historical data, skip CHRC
            correction = torch.zeros_like(adapted_pred)
            chrc_confidence = torch.zeros(batch_size, 1, device=x_enc.device)
        else:
            correction, chrc_confidence = self.chrc(adapted_pred, pogt, return_confidence=True)
            correction = correction - adapted_pred  # Get the delta only
        outputs['correction'] = correction

        # Step 5: Fusion - Combine adapted prediction with correction
        pred_flat = adapted_pred.reshape(batch_size, -1)
        corr_flat = correction.reshape(batch_size, -1)
        fusion_input = torch.cat([pred_flat, corr_flat, memory_state], dim=-1)
        fusion_weight = self.fusion_gate(fusion_input)  # [batch, 1]

        # Modulate by CHRC confidence
        effective_weight = fusion_weight * chrc_confidence
        outputs['fusion_weight'] = effective_weight

        # Final prediction
        final_pred = adapted_pred + effective_weight.unsqueeze(-1) * correction
        outputs['prediction'] = final_pred

        # Store for delayed memory bank update
        self._last_pogt = pogt.detach()
        self._last_prediction = final_pred.detach()

        return outputs

    def update_memory_bank(self, ground_truth: torch.Tensor):
        """
        Update error memory bank when full ground truth becomes available.

        Called H steps after prediction, when GT is revealed.

        Args:
            ground_truth: Full horizon ground truth [batch, pred_len, features]
        """
        if self._last_pogt is None or self._last_prediction is None:
            return

        # Compute prediction error
        error = ground_truth - self._last_prediction

        # Store in CHRC memory bank
        self.chrc.store_error(self._last_pogt, error)

        # Update cold start flag
        self._is_cold_start = torch.tensor(False)

        # Clear cache
        self._last_pogt = None
        self._last_prediction = None

    def reset_memory(self):
        """Reset all memory states (for new sequence)."""
        self.snma.reset(batch_size=1)
        self.chrc.memory_bank.clear()
        self._is_cold_start = torch.tensor(True)
        self._last_pogt = None
        self._last_prediction = None

    def get_param_stats(self) -> dict:
        """Get parameter statistics for monitoring."""
        stats = self._param_stats.copy()
        stats['memory_bank'] = self.chrc.memory_bank.get_stats()
        return stats

    def freeze_snma(self, freeze: bool = True):
        """Freeze/unfreeze SNMA parameters."""
        for param in self.snma.parameters():
            param.requires_grad = not freeze

    def freeze_chrc(self, freeze: bool = True):
        """Freeze/unfreeze CHRC parameters."""
        for param in self.chrc.parameters():
            param.requires_grad = not freeze

    def freeze_fusion(self, freeze: bool = True):
        """Freeze/unfreeze fusion gate parameters."""
        for param in self.fusion_gate.parameters():
            param.requires_grad = not freeze


def build_hmem(backbone: nn.Module, args) -> HMem:
    """
    Factory function to build H-Mem model.

    Args:
        backbone: Pre-trained backbone model
        args: Configuration arguments

    Returns:
        Initialized H-Mem model
    """
    return HMem(backbone, args)
```

---

### 4.5 Experiment Class

**File:** `exp/exp_hmem.py`

```python
"""
Experiment class for H-Mem training and evaluation.
"""

import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from typing import Optional, Dict, Any
import numpy as np
from tqdm import tqdm

from exp.exp_online import Exp_Online
from adapter.hmem import HMem, build_hmem


class Exp_HMem(Exp_Online):
    """
    H-Mem experiment class.

    Extends Exp_Online with H-Mem specific:
    - Two-phase training (SNMA warmup + joint training)
    - Delayed memory bank updates
    - POGT extraction from streaming data
    """

    def __init__(self, args):
        # Set H-Mem as the framework class
        args.framework_class = HMem
        super().__init__(args)

        # H-Mem specific settings
        self.pogt_ratio = getattr(args, 'pogt_ratio', 0.5)
        self.warmup_steps = getattr(args, 'hmem_warmup_steps', 100)
        self.joint_training = getattr(args, 'hmem_joint_training', True)

        # Delayed update buffer: stores (pogt, prediction) waiting for GT
        self.pending_updates = []
        self.delay_steps = args.pred_len  # Wait for full horizon

    def _build_model(self, model=None, framework_class=None):
        """Build model with H-Mem wrapper."""
        # Build backbone first
        backbone = super()._build_model(model, framework_class=None)

        # Wrap with H-Mem
        hmem_model = build_hmem(backbone, self.args)

        if self.args.use_multi_gpu and self.args.use_gpu:
            hmem_model = nn.DataParallel(hmem_model, device_ids=self.args.device_ids)

        return hmem_model.to(self.device)

    def _select_optimizer(self, filter_frozen: bool = True):
        """
        Create optimizer with different learning rates for components.
        """
        model = self._model.module if hasattr(self._model, 'module') else self._model

        # Group parameters
        snma_params = list(model.snma.parameters())
        chrc_params = list(model.chrc.parameters())
        fusion_params = list(model.fusion_gate.parameters())

        param_groups = [
            {'params': snma_params, 'lr': self.args.online_learning_rate},
            {'params': chrc_params, 'lr': self.args.online_learning_rate * 0.5},
            {'params': fusion_params, 'lr': self.args.online_learning_rate * 0.5}
        ]

        return AdamW(param_groups, weight_decay=getattr(self.args, 'weight_decay', 0.01))

    def _extract_pogt(self, recent_batch: torch.Tensor) -> torch.Tensor:
        """
        Extract POGT from recent observed data.

        Args:
            recent_batch: Recently observed ground truth [batch, recent_len, features]

        Returns:
            POGT tensor [batch, pogt_len, features]
        """
        model = self._model.module if hasattr(self._model, 'module') else self._model
        pogt_len = model.pogt_len

        # Take the last pogt_len steps as POGT
        if recent_batch.size(1) >= pogt_len:
            return recent_batch[:, -pogt_len:]
        else:
            # Pad if needed
            padding = torch.zeros(
                recent_batch.size(0),
                pogt_len - recent_batch.size(1),
                recent_batch.size(2),
                device=recent_batch.device
            )
            return torch.cat([padding, recent_batch], dim=1)

    def _update_online(
        self,
        batch: tuple,
        criterion: nn.Module,
        optimizer: torch.optim.Optimizer,
        scaler: Optional[torch.cuda.amp.GradScaler] = None
    ) -> float:
        """
        Online update step for H-Mem.

        Args:
            batch: (batch_x, batch_y, batch_x_mark, batch_y_mark)
            criterion: Loss function
            optimizer: Optimizer
            scaler: Optional AMP scaler

        Returns:
            Loss value
        """
        self._model.train()
        model = self._model.module if hasattr(self._model, 'module') else self._model

        # Unpack batch
        batch_x, batch_y, batch_x_mark, batch_y_mark = batch
        batch_x = batch_x.float().to(self.device)
        batch_y = batch_y.float().to(self.device)

        if batch_x_mark is not None:
            batch_x_mark = batch_x_mark.float().to(self.device)

        # Extract POGT from recent observations
        # In online setting, POGT comes from the overlap between previous prediction
        # and current observation window
        pogt = self._extract_pogt(batch_y[:, :model.pogt_len])

        optimizer.zero_grad()

        # Forward pass
        with torch.cuda.amp.autocast(enabled=scaler is not None):
            outputs = model(batch_x, batch_x_mark, pogt=pogt, mode='adapt')
            prediction = outputs['prediction']

            # Main loss: prediction vs ground truth
            loss = criterion(prediction, batch_y[:, -self.args.pred_len:])

            # Auxiliary loss: adapted prediction should be better than base
            if 'base_prediction' in outputs and 'adapted_prediction' in outputs:
                base_loss = criterion(outputs['base_prediction'], batch_y[:, -self.args.pred_len:])
                adapted_loss = criterion(outputs['adapted_prediction'], batch_y[:, -self.args.pred_len:])

                # Regularization: encourage adaptation to improve
                adaptation_bonus = torch.relu(adapted_loss - base_loss) * 0.1
                loss = loss + adaptation_bonus

        # Backward pass
        if scaler is not None:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()

        # Store for delayed memory bank update
        self.pending_updates.append({
            'pogt': pogt.detach(),
            'prediction': outputs['prediction'].detach(),
            'step': self._current_step
        })

        return loss.item()

    def _process_delayed_updates(self, current_step: int, ground_truth: torch.Tensor):
        """
        Process pending memory bank updates when GT becomes available.

        Args:
            current_step: Current online step
            ground_truth: Full ground truth that just became available
        """
        model = self._model.module if hasattr(self._model, 'module') else self._model

        # Find updates that are now ready (enough delay has passed)
        ready_updates = [
            u for u in self.pending_updates
            if current_step - u['step'] >= self.delay_steps
        ]

        for update in ready_updates:
            model.update_memory_bank(ground_truth)
            self.pending_updates.remove(update)

    def online(
        self,
        online_data,
        target_variate: int = 0,
        phase: str = 'test'
    ) -> Dict[str, Any]:
        """
        Main online learning loop for H-Mem.

        Args:
            online_data: Online dataset
            target_variate: Target variable index
            phase: 'val' or 'test'

        Returns:
            Dictionary with metrics and predictions
        """
        model = self._model.module if hasattr(self._model, 'module') else self._model
        criterion = self._get_criterion()
        optimizer = self._select_optimizer()

        # Optional: learning rate scheduler
        scheduler = CosineAnnealingLR(
            optimizer,
            T_max=len(online_data) // self.args.batch_size,
            eta_min=self.args.online_learning_rate * 0.01
        )

        # Reset memory for new online phase
        model.reset_memory()
        self.pending_updates = []
        self._current_step = 0

        # Tracking
        preds, trues = [], []
        losses = []
        memory_stats = []

        # Create dataloader
        online_loader = self._get_online_loader(online_data)

        # Two-phase training
        warmup_phase = True

        for i, (recent_batch, current_batch) in enumerate(tqdm(online_loader, desc=f'H-Mem Online ({phase})')):
            self._current_step = i

            # Phase switching
            if warmup_phase and i >= self.warmup_steps:
                warmup_phase = False
                if self.joint_training:
                    # Unfreeze CHRC after warmup
                    model.freeze_chrc(False)
                print(f"[H-Mem] Warmup complete at step {i}. Switching to joint training.")

            if warmup_phase:
                # Only train SNMA during warmup
                model.freeze_chrc(True)

            # Update model with recent observed data
            if recent_batch is not None:
                loss = self._update_online(recent_batch, criterion, optimizer)
                losses.append(loss)

                # Process delayed memory bank updates
                if len(self.pending_updates) > 0:
                    self._process_delayed_updates(i, recent_batch[1])

            # Make prediction for current window
            with torch.no_grad():
                model.eval()
                batch_x, batch_y, batch_x_mark, batch_y_mark = current_batch
                batch_x = batch_x.float().to(self.device)
                batch_y = batch_y.float().to(self.device)

                if batch_x_mark is not None:
                    batch_x_mark = batch_x_mark.float().to(self.device)

                # Use POGT if available
                pogt = self._extract_pogt(batch_y[:, :model.pogt_len]) if batch_y.size(1) > model.pogt_len else None

                outputs = model(batch_x, batch_x_mark, pogt=pogt, mode='predict')
                pred = outputs['prediction']

            preds.append(pred.cpu().numpy())
            trues.append(batch_y[:, -self.args.pred_len:].cpu().numpy())

            # Track memory stats periodically
            if i % 100 == 0:
                memory_stats.append(model.get_param_stats())

            # Step scheduler
            scheduler.step()

        # Aggregate results
        preds = np.concatenate(preds, axis=0)
        trues = np.concatenate(trues, axis=0)

        # Compute metrics
        from util.metrics import metric
        mae, mse, rmse, mape, mspe = metric(preds, trues)

        results = {
            'mae': mae,
            'mse': mse,
            'rmse': rmse,
            'mape': mape,
            'mspe': mspe,
            'avg_loss': np.mean(losses),
            'predictions': preds,
            'ground_truth': trues,
            'memory_stats': memory_stats
        }

        print(f"[H-Mem] {phase} Results: MSE={mse:.4f}, MAE={mae:.4f}")

        return results

    def update_valid(self, valid_data):
        """
        Adapt on validation set before testing.

        Uses validation data to warm up the neural memory
        before entering the test phase.
        """
        print("[H-Mem] Warming up on validation data...")
        model = self._model.module if hasattr(self._model, 'module') else self._model

        # Reset memory
        model.reset_memory()

        # Run through validation data to populate memory
        _ = self.online(valid_data, phase='val')

        print("[H-Mem] Validation warmup complete.")
```

---

### 4.6 Configuration Updates

**File:** `settings.py` (additions)

```python
# Add to hyperparams dictionary
hyperparams['HMem'] = {
    # LoRA settings
    'lora_rank': 8,
    'lora_alpha': 16.0,
    'lora_target_modules': None,  # None = all Linear/Conv1d layers

    # Neural memory settings
    'memory_dim': 256,
    'bottleneck_dim': 32,
    'memory_momentum': 0.9,

    # Error memory bank settings
    'memory_capacity': 1000,
    'retrieval_top_k': 5,
    'chrc_feature_dim': 128,
    'chrc_temperature': 0.1,

    # POGT settings
    'pogt_ratio': 0.5,  # Use 50% of horizon as POGT

    # Training settings
    'hmem_warmup_steps': 100,
    'hmem_joint_training': True,
    'weight_decay': 0.01,
}

# Add learning rates
pretrain_lr_online_dict['HMem'] = 0.0001
```

**File:** `run.py` (additions)

```python
# Add to argument parser
parser.add_argument('--lora_rank', type=int, default=8, help='LoRA rank')
parser.add_argument('--lora_alpha', type=float, default=16.0, help='LoRA scaling factor')
parser.add_argument('--memory_dim', type=int, default=256, help='Neural memory dimension')
parser.add_argument('--memory_capacity', type=int, default=1000, help='Error memory bank capacity')
parser.add_argument('--retrieval_top_k', type=int, default=5, help='Top-K for retrieval')
parser.add_argument('--pogt_ratio', type=float, default=0.5, help='POGT ratio')
parser.add_argument('--hmem_warmup_steps', type=int, default=100, help='SNMA warmup steps')

# Add to online_method choices
# In the argument definition for --online_method, add 'HMem' to choices
```

**File:** `exp/__init__.py` (additions)

```python
# Add lazy loading for Exp_HMem
def __getattr__(name):
    if name == 'Exp_HMem':
        from exp.exp_hmem import Exp_HMem
        return Exp_HMem
    # ... existing lazy loading
```

---

## 5. Implementation Roadmap

### Phase 1: Core Infrastructure (Week 1)

| Task | File | Priority | Dependencies |
|------|------|----------|--------------|
| LoRA layer implementation | `adapter/module/lora.py` | P0 | None |
| Unit tests for LoRA | `tests/test_lora.py` | P0 | lora.py |
| Neural memory module | `adapter/module/neural_memory.py` | P0 | None |
| Unit tests for memory | `tests/test_neural_memory.py` | P0 | neural_memory.py |

### Phase 2: Memory Bank & Retrieval (Week 2)

| Task | File | Priority | Dependencies |
|------|------|----------|--------------|
| Error memory bank | `util/error_bank.py` | P0 | None |
| CHRC module | `util/error_bank.py` | P0 | error_bank |
| Integration tests | `tests/test_chrc.py` | P1 | CHRC |

### Phase 3: H-Mem Integration (Week 3)

| Task | File | Priority | Dependencies |
|------|------|----------|--------------|
| H-Mem main module | `adapter/hmem.py` | P0 | Phase 1 & 2 |
| Experiment class | `exp/exp_hmem.py` | P0 | hmem.py |
| Config updates | `settings.py`, `run.py` | P1 | hmem.py |
| Lazy loading | `exp/__init__.py` | P1 | exp_hmem.py |

### Phase 4: Validation & Optimization (Week 4)

| Task | Description | Priority |
|------|-------------|----------|
| ETTh1 baseline | Run on ETTh1 to verify correctness | P0 |
| Hyperparameter sweep | LoRA rank, memory dim, top-K | P1 |
| Memory efficiency | Profile and optimize memory usage | P1 |
| Multi-GPU testing | Verify DataParallel works | P2 |

### Phase 5: Benchmarking (Week 5+)

| Task | Description | Priority |
|------|-------------|----------|
| Full benchmark | All 8 standard datasets | P0 |
| Ablation study | SNMA only, CHRC only, full H-Mem | P0 |
| Comparison | vs PROCEED, TAFAS, FSNet, OneNet | P0 |
| Analysis | Memory bank utilization, adaptation curves | P1 |

---

## 6. Testing Strategy

### Unit Tests

```python
# tests/test_lora.py
def test_lora_linear_forward():
    """Test LoRA linear layer produces correct output shape."""

def test_lora_param_injection():
    """Test dynamic LoRA parameter injection works."""

def test_lora_gradient_flow():
    """Test gradients flow through LoRA layers correctly."""

# tests/test_neural_memory.py
def test_memory_update():
    """Test memory state updates with new information."""

def test_surprise_calculation():
    """Test surprise score computation."""

def test_memory_reset():
    """Test memory state reset functionality."""

# tests/test_chrc.py
def test_error_bank_store_retrieve():
    """Test storing and retrieving from error bank."""

def test_eviction_policy():
    """Test intelligent eviction when at capacity."""

def test_temporal_decay():
    """Test time-based decay of similarities."""
```

### Integration Tests

```python
# tests/test_hmem_integration.py
def test_hmem_forward_cold_start():
    """Test H-Mem forward pass during cold start (empty memory)."""

def test_hmem_forward_with_pogt():
    """Test H-Mem forward pass with POGT available."""

def test_hmem_memory_bank_update():
    """Test delayed memory bank update mechanism."""

def test_hmem_online_loop():
    """Test complete online learning loop on small dataset."""
```

### Run Commands

```bash
# Unit tests
pytest tests/test_lora.py -v
pytest tests/test_neural_memory.py -v
pytest tests/test_chrc.py -v

# Integration tests
pytest tests/test_hmem_integration.py -v

# Quick validation on ETTh1
python run.py --model PatchTST --dataset ETTh1 --online_method HMem \
    --seq_len 96 --pred_len 24 --lora_rank 8 --memory_capacity 500 \
    --hmem_warmup_steps 50 --itr 1

# Full benchmark
bash scripts/online/HMem/benchmark_all.sh
```

---

## 7. Key Design Decisions & Rationale

### 7.1 Why LoRA over SSF/Down-Up?

| Aspect | SSF/Down-Up (PROCEED) | LoRA (H-Mem) |
|--------|----------------------|--------------|
| **Parameter efficiency** | ~3N params per layer | ~2rN params (r << N) |
| **Interpretability** | Scale + Shift | Low-rank weight delta |
| **Generation complexity** | Simple MLP | Slightly more complex |
| **Flexibility** | Limited to affine transform | Full weight modification |

**Decision:** LoRA provides better parameter efficiency and more expressive power for capturing complex adaptation patterns.

### 7.2 Why Separate SNMA and CHRC?

**Rationale:** They address different types of drift:
- **SNMA** (parametric): Handles **covariate shift** - when input distribution changes but the mapping remains similar
- **CHRC** (non-parametric): Handles **concept drift** - when the input-output mapping itself changes

Combining both provides robustness against both types of non-stationarity.

### 7.3 Why Delayed Memory Bank Update?

**Rationale:** In real online TSF, we don't have the full prediction error until H steps later. Storing errors immediately would be information leakage. The delayed update mechanism ensures the framework respects the temporal causality of real-world forecasting.

### 7.4 Why Fusion Gate instead of Simple Addition?

**Rationale:** SNMA-adapted prediction and CHRC correction may conflict. A learned fusion gate can:
1. Detect when CHRC retrieval is unreliable (low similarity)
2. Weigh contributions based on context
3. Avoid over-correction when both modules push in same direction

---

## 8. Potential Extensions

### 8.1 Multi-Scale Memory

```python
# Future: Hierarchical memory at different time scales
class MultiScaleMemory(nn.Module):
    def __init__(self):
        self.short_term = NeuralMemoryState(memory_dim=128)  # ~10 steps
        self.medium_term = NeuralMemoryState(memory_dim=256)  # ~100 steps
        self.long_term = NeuralMemoryState(memory_dim=512)   # ~1000 steps
```

### 8.2 Adaptive Retrieval

```python
# Future: Learn when to retrieve
class AdaptiveRetrieval(nn.Module):
    def should_retrieve(self, context):
        """Predict whether retrieval would be beneficial."""
        return self.gate(context) > 0.5
```

### 8.3 Continual Backbone Adaptation

```python
# Future: Occasionally update frozen backbone with accumulated knowledge
def consolidate_lora_to_backbone(self, consolidation_rate=0.01):
    """Slowly merge LoRA into backbone weights."""
    for name, module in self.backbone.named_modules():
        if isinstance(module, LoRALinear):
            delta = module.lora_A @ module.lora_B * module.scaling
            module.weight.data += consolidation_rate * delta
```

---

## 9. Appendix: Mathematical Formulation

### 9.1 SNMA Update Rule

$$
\begin{aligned}
\text{Surprise: } & s_t = \sqrt{\mathbb{E}[(f(X_{t-P:t}) - Y_{t-P:t})^2]} \\
\text{Encoding: } & h_t = \text{Encoder}(Y_{t-P:t}) \\
\text{Forget gate: } & f_t = \sigma(W_f[M_{t-1}; h_t]) \\
\text{Update gate: } & u_t = \sigma(W_u[M_{t-1}; h_t; s_t]) \\
\text{Content: } & \tilde{c}_t = \tanh(W_c \cdot h_t) \\
\text{Memory: } & M_t = f_t \odot M_{t-1} + u_t \odot \tilde{c}_t \\
\text{LoRA params: } & \{\mathbf{A}_l, \mathbf{B}_l\}_{l=1}^L = \text{HyperNet}(M_t)
\end{aligned}
$$

### 9.2 CHRC Retrieval

$$
\begin{aligned}
\text{Query: } & q = \text{FeatureEnc}(Y_{t-P:t}) \\
\text{Similarity: } & s_i = \frac{q \cdot k_i}{\|q\| \|k_i\|} \cdot \gamma^{t - t_i} \\
\text{Top-K: } & \mathcal{K} = \text{argtop}_K(s_i) \\
\text{Aggregate: } & \hat{E} = \sum_{i \in \mathcal{K}} \text{softmax}(s_i / \tau) \cdot v_i
\end{aligned}
$$

### 9.3 Final Fusion

$$
\begin{aligned}
\hat{Y}_{adapt} &= F_{\theta + \Delta\theta}(X_t) \\
\hat{Y}_{final} &= \hat{Y}_{adapt} + \lambda([\hat{Y}_{adapt}; \hat{E}; M_t]) \cdot \hat{E}
\end{aligned}
$$

Where $\lambda(\cdot) \in [0, 1]$ is the learned fusion gate.

---

## 10. Conclusion

本文档提供了 H-Mem 在 OnlineTSF 框架中的完整实现方案，包括：

1. **架构设计**：基于 LoRA 的轻量级适配 + 双重记忆机制
2. **代码实现**：6 个核心模块的详细实现
3. **配置集成**：与现有框架无缝集成
4. **测试策略**：单元测试 + 集成测试
5. **开发路线**：5 阶段实施计划

核心创新点：
- **神经记忆驱动的 LoRA 生成**：将 Titans 的神经记忆与参数高效微调结合
- **检索增强的误差修正**：利用历史误差模式解决反馈延迟问题
- **学习的融合机制**：智能组合两种适应机制

建议按照 Phase 1 → Phase 5 的顺序实施，确保每个模块经过充分测试后再进行集成。

---

*Document generated by Claude Code Assistant*
*Last updated: 2025-12-13*
